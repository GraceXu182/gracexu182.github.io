---
date: 2023-9-21
title: New Paper on "Norm-Based Generalization Bounds for Compositionally Sparse Neural Networks"
# subtitle: Norm-Based Generalization Bounds for Compositionally Sparse Neural Networks
description: Paper titled "Norm-Based Generalization Bounds for Compositionally Sparse Neural Networks" has been accepted by NeurIPS.
event_type: publication # primary type of the event, used to create the small, colored post callout
banner_image: assets/images/seminar_banner2.jpg # the item whose banner image will be adopted by this event
badges: # e.g. person=John_Doe, project=project1 (no spaces)
  # - publication=Hyperbolic_brainaging_2024
  # - project=project2
#   - person=Cole_Baker
  # - project=brain_aging_study
---
 
<a href="../#latest-news"><b>&lt;&lt;All News</b></a>

We are excited to share our latest accepted *NeurIPS* paper, a collaboration with esteemed colleagues from <b>MIT</b>.

**Title:** **<span style="color: darkred">Norm-Based Generalization Bounds for Compositionally Sparse Neural Networks</span>**  
**Authors:** **<span style="color: darkred">Tomer Galanti, Mengjia Xu, Liane Galanti, Tomaso Poggio*</span>**  
**Links:** [Download paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/8493e190ff1bbe3837eca821190b61ff-Paper-Conference.pdf) | <a href="#" id="bibtex" onclick="navigator.clipboard.writeText('@article{galanti2024norm, \n title={Norm-based generalization bounds for sparse neural networks}, \n author={Galanti, Tomer and Xu, Mengjia and Galanti, Liane and Poggio, Tomaso}, \n journal={Advances in Neural Information Processing Systems}, \n volume={36}, \n year={2024}}'); alert('BibTex copied to clipboard!');">BibTex</a>

#### Overview

<p align="justify">In this work, we present new norm-based generalization bounds for compositionally sparse neural networks, such as convolutional networks. Our main contributions include deriving significantly tighter bounds for sparse networks without relying on weight sharing and demonstrating that good generalization can be achieved with weak dependence on network size. Our experiments confirm that these bounds outperform existing ones, emphasizing the importance of network architecture on test performance. 
<b><span style="color: darkred">This work sheds light on why certain architectures excel, suggesting that sparsity is a key factor in their success</span></b>.
</p>

<div style="display: flex; flex-wrap: wrap; margin-top: 10px; justify-content: center; gap: 10px;">
    <img src="../../assets/images/projects/bound_comparison.png" alt="Figure 1" style="width: 80%; height: auto;">
    <img src="../../assets/images/projects/varyinglambda.png" alt="Figure 2" style="width: 80%; height: auto;">
    <img src="../../assets/images/projects/varyingchannels.png" alt="Figure 3" style="width: 80%; height: auto;">
    <img src="../../assets/images/projects/varyinglayerNum.png" alt="Figure 4" style="width: 80%; height: auto;">
</div>

<div style="width: 100%; padding: 10px; ">
  <div style="display: flex; justify-content: space-between;">
    <a href="../post5/">&lt;&lt;Previous News</a>
    <a href="../post6/">Next News&gt;&gt;</a>
  </div>
</div>