---
date: 2025-04-29
title: Thesis Defense by Ashish Pandey
subtitle: Congratulations to Ashish for successfully defending his Master's thesis! ðŸŽ‰
description: Ashish successfully defended his Master's Thesis titled <a href = ''> "A novel framework for ."</a>
event_type: defense # primary type of the event, used to create the small, colored post callout
banner_image: assets/images/seminar_banner2.jpg # the item whose banner image will be adopted by this event
badges: # e.g. person=John_Doe, project=project1 (no spaces)
  - project=project1
  - person=Ashish_Pandey
---

<a href="../#latest-news"><b>&lt;&lt;All News</b></a>


<p align = 'justify'>
Join us in celebrating Ashish Pandey's Master's Thesis Defense. He will present his research titled <a href = ''>"A NOVEL FRAMEWORK FOR DYNAMIC GRAPH
REPRESENTATION LEARNING WITH MAMBA"</a>. This study highlights the use of Mamba model for effective and efficient temporal graph representation learning with uncertainty quantification.
</p>

**Date and Time:** Tuesday, April 29, 2025
**Time:** 10:00 AM to 11:00 AM  
**Location:** NJIT Campus, GITC Room 2111  
**Zoom Link:** [Join Here](https://njit-edu.zoom.us/j/93528086169?pwd=hFD3aabJdD9gwIOjxyItX9qoi3Up)

### Abstract
<p align = 'justify'>
Dynamic graph embedding has emerged as an important technique for modeling complex time-evolving networks across diverse domains. While transformer-based models have shown promise in capturing long-range dependencies in temporal graph data, they face scalability challenges due to quadratic computational complexity. This study presents a comparative analysis of dynamic graph embedding approaches using transformers and the recently proposed Mamba architecture, a state-space model with linear complexity. We introduce three novel models: TransformerG2G augmented with graph convolutional networks, $\mathcal{DG}$-Mamba, and $\mathcal{GDG}$-Mamba with graph isomorphism network edge convolutions. Our experiments on multiple benchmark datasets demonstrate that Mamba-based models achieve comparable or superior performance to transformer-based approaches in link prediction tasks while offering significant computational efficiency gains on longer sequences. Notably, $\mathcal{DG}$-Mamba variants consistently outperform transformer-based models on datasets with high temporal variability, such as UCI, Bitcoin-OTC, and Reality Mining, while maintaining competitive performance on more stable graphs like SBM. We provide insights into the learned temporal dependencies through analysis of attention weights and state matrices, revealing the models' ability to capture complex temporal patterns. By effectively combining state-space models with graph neural networks, our work addresses key limitations of previous approaches and contributes to the growing body of research on efficient temporal graph representation learning. These findings offer promising directions for scaling dynamic graph embedding to larger, more complex real-world networks, potentially enabling new applications in areas such as social network analysis, financial modeling, and biological system dynamics.
</p>


<div style="width: 100%; padding: 10px; border: 1px solid #ccc; background-color: #f8f8f8;">
  <div style="display: flex; justify-content: space-between;">
    <a href="../post1/">&lt;&lt;Previous News</a>
    <a href="../post2/">Next News&gt;&gt;</a>
  </div>
</div>
